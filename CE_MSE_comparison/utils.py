import numpy as np
import pandas as pd
import time
from sklearn import datasets
from scipy import optimize
import matplotlib.pyplot as plt


from sklearn.model_selection import train_test_split

def sigmoid(z):
    return 1/(1+np.exp(-z))

def sigmoid_grad(z):
    return sigmoid(z)*(1-sigmoid(z))

def results(X,y, theta):
    # compute sigmoid values/pred probabilities using given(optimal) theta
    pred_test = sigmoid(np.matmul(theta, X.T))
    # compute predicted class/label
    pred_y_test = np.zeros(y_test.shape)
    pred_y_test[pred_test >= 0.5] = 1
    # compute confusion matrix values
    true_values = np.sum(y_test ==  pred_y_test)
    FN = np.sum((y_test == 1) & (pred_y_test == 0))
    FP = np.sum((y_test == 0) & (pred_y_test == 1))
    TN =  np.sum((y_test == 0) & (pred_y_test == 0))
    TP = np.sum((y_test == 1) & (pred_y_test == 1))
    # report results
    print('False negatives ', FN)
    print('false positive ', FP)
    print('true negatives ', TN)
    print('true positives ', TP)
    print('total true values ', TN+TP)
    print('\n')
    precision = TP/(TP+FP)
    recall = TP/(TP+FN)
    print('precision {0:1.4f}'.format(precision))
    print('recall {0:1.4f}'.format(recall))
    print('accuracy % 2.4f ' % (true_values/len(y_test)))
    print('F1 score % 1.4f ' % (2*precision*recall/(precision+recall)))
    
    
 # slices of cross entropy loss
# this function creates data needed for plotting profiles/slices

def get_profile(costFunc,X_train_o, y_train, th1, th2, ths1, ths2, axis):
    '''
     Parameters:
     costFunc: the name of the cost function
     th1 : values of theta_0 used to plot the profile
     th2 : values of theta_1 used to plot the profile
     ths1 & ths2 : values of theta_0 and theta_1 respectively
                   generated by the gradient descent alogorithm
                   along the path of convergence to optimal theta
     axis: detemines the direction of the slice. 
           expected values are 'theta_0' or 'theta_1'
     Returns:
      J_t1: values of the cost function plotting the profile
      J_tt1 values of the cost function plotting the points 
            generated by the gradient descent algorithm.
    '''
    if axis == 'theta_0':
        J_t1 = np.zeros(th1.shape)
        for i in range(len(th1)):
            th = np.array([th1[i],th2])
            J_t1[i], _ = costFunc(th,X_train_o,y_train)

        J_tt1 = np.zeros(len(ths1))
        for i, t1 in enumerate(ths1):
            th = np.array([t1,th2])
            J_tt1[i], _ =costFunc(th,X_train_o,y_train)
        return J_t1, J_tt1
    
    elif axis == 'theta_1':
        J_t2 = np.zeros(th2.shape)
        for i in range(len(th2)):
            th = np.array([th1,th2[i]])
            J_t2[i], _ = costFunc(th, X_train_o,y_train)

        J_tt2 = np.zeros(len(ths2))
        for i, t2 in enumerate(ths2):
            th = np.array([th1,t2])
            J_tt2[i], _ =costFunc(th,X_train_o,y_train)
        return J_t2, J_tt2
    
    
    
# preparing plots of profiles
def plot_profiles(th1_1, th2_2, ths1, ths2, J_t1, J_tt1, J_t2, J_tt2, loss_type = 'CE'):
    plt.subplots(1,2,figsize =(12,4))

    plt.subplot(121)
    plt.plot(th1_1, J_t1)
    plt.plot(ths1, J_tt1, 'r*')
    plt.title('profile along theta_0')
    plt.xlabel('theta_0')
    plt.ylabel('Loss')


    plt.subplot(122)
    plt.plot(th2_2, J_t2)
    plt.plot(ths2, J_tt2,'*r')
    plt.title('profile along theta_1')
    plt.xlabel('theta_1')
    plt.ylabel('Loss')

    plt.suptitle('Profile of the '+str(loss_type)+' loss function \n in red are the theta values of few iterations as projected on the profile')
    plt.tight_layout()
    plt.show()
    
  